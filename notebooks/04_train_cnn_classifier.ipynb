{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Classifier Training (Supervised Baseline)\n",
    "\n",
    "Trains a lightweight CNN for supervised defect classification on **NEU Surface Defect** dataset.\n",
    "\n",
    "**Key Info:**\n",
    "- 6 defect classes: crazing, inclusion, patches, pitted_surface, rolled-in_scale, scratches\n",
    "- Supervised classification (not anomaly detection)\n",
    "- Used as baseline to compare with unsupervised methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'F:/Thesis')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "from src.config import DEVICE, MODELS_DIR, FIGURES_DIR, NEU_CATEGORIES, ensure_dirs\n",
    "from src.data import NEUDataset\n",
    "from src.models import create_cnn_classifier\n",
    "from src.training import get_optimizer, get_scheduler\n",
    "\n",
    "ensure_dirs()\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Classes: {NEU_CATEGORIES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'batch_size': 16,\n",
    "    'num_epochs': 50,\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'num_classes': 6\n",
    "}\n",
    "\n",
    "train_dataset = NEUDataset(split='train')\n",
    "val_dataset = NEUDataset(split='validation')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=0\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_cnn_classifier(num_classes=CONFIG['num_classes']).to(DEVICE)\n",
    "optimizer = get_optimizer(model, lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
    "scheduler = get_scheduler(optimizer, patience=3, factor=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in tqdm(range(1, CONFIG['num_epochs'] + 1), desc='Training'):\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for img, label in train_loader:\n",
    "        img, label = img.to(DEVICE), label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(img)\n",
    "        loss = criterion(logits, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validate\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for img, label in val_loader:\n",
    "            img, label = img.to(DEVICE), label.to(DEVICE)\n",
    "            logits = model(img)\n",
    "            val_loss += criterion(logits, label).item()\n",
    "            correct += (logits.argmax(1) == label).sum().item()\n",
    "    \n",
    "    avg_train = train_loss / len(train_loader)\n",
    "    avg_val = val_loss / len(val_loader)\n",
    "    acc = correct / len(val_dataset)\n",
    "    \n",
    "    history['train_loss'].append(avg_train)\n",
    "    history['val_loss'].append(avg_val)\n",
    "    history['val_acc'].append(acc)\n",
    "    \n",
    "    scheduler.step(avg_val)\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Train Loss={avg_train:.4f}, Val Loss={avg_val:.4f}, Val Acc={acc:.4f}\")\n",
    "\n",
    "print(f\"\\nBest validation accuracy: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "axes[0].plot(epochs, history['train_loss'], 'b-', label='Train')\n",
    "axes[0].plot(epochs, history['val_loss'], 'r-', label='Validation')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss Curves')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(epochs, history['val_acc'], 'g-', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Validation Accuracy')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('CNN Classifier Training - NEU Surface Defect', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'cnn_training_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix & Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img, label in val_loader:\n",
    "        logits = model(img.to(DEVICE))\n",
    "        preds = logits.argmax(1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(label.numpy())\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=NEU_CATEGORIES, yticklabels=NEU_CATEGORIES, cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix - CNN Classifier')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'cnn_confusion_matrix.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(all_labels, all_preds, target_names=NEU_CATEGORIES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = MODELS_DIR / 'cnn_classifier_final.pth'\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': CONFIG,\n",
    "    'history': history,\n",
    "    'accuracy': best_acc,\n",
    "}, save_path)\n",
    "\n",
    "print(f\"Model saved to: {save_path}\")\n",
    "print(f\"\\n=== Training Summary ===\")\n",
    "print(f\"Epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"Best Accuracy: {best_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}},
 "nbformat": 4,
 "nbformat_minor": 4
}
