{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Analysis & Visualization Notebook\n",
    "\n",
    "Comprehensive visualizations for anomaly detection research:\n",
    "- Dataset exploration\n",
    "- Model comparison\n",
    "- ROC/PR curves\n",
    "- Anomaly heatmaps\n",
    "- Score distributions\n",
    "- Per-category analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'F:/Thesis')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from src.config import DEVICE, MODELS_DIR, FIGURES_DIR, MVTEC_CATEGORIES, ensure_dirs\n",
    "from src.data import MVTecDataset\n",
    "from src.data.transforms import denormalize\n",
    "from src.models import create_cae, create_vae, create_denoising_ae\n",
    "from src.evaluation import (\n",
    "    set_style, plot_reconstruction_grid, plot_anomaly_heatmap_overlay,\n",
    "    plot_roc_curves, plot_precision_recall_curves, plot_score_distribution,\n",
    "    plot_category_comparison, plot_latent_space_2d, plot_training_curves\n",
    ")\n",
    "\n",
    "ensure_dirs()\n",
    "set_style()\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize samples from each MVTec category\n",
    "fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
    "for i, cat in enumerate(MVTEC_CATEGORIES[:15]):\n",
    "    try:\n",
    "        ds = MVTecDataset(category=cat, split='train')\n",
    "        img, _ = ds[0]\n",
    "        img_np = denormalize(img).permute(1, 2, 0).numpy().clip(0, 1)\n",
    "        axes[i//5, i%5].imshow(img_np)\n",
    "        axes[i//5, i%5].set_title(cat.title(), fontsize=10)\n",
    "        axes[i//5, i%5].axis('off')\n",
    "    except Exception as e:\n",
    "        axes[i//5, i%5].text(0.5, 0.5, 'N/A', ha='center')\n",
    "        axes[i//5, i%5].axis('off')\n",
    "\n",
    "plt.suptitle('MVTec AD Categories (15 Types)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'mvtec_categories_overview.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics\n",
    "stats = []\n",
    "for cat in MVTEC_CATEGORIES:\n",
    "    try:\n",
    "        train_ds = MVTecDataset(category=cat, split='train')\n",
    "        test_ds = MVTecDataset(category=cat, split='test')\n",
    "        n_defect = sum(test_ds.labels)\n",
    "        stats.append({'category': cat, 'train': len(train_ds), 'test': len(test_ds), \n",
    "                      'normal': len(test_ds) - n_defect, 'defect': n_defect})\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "import pandas as pd\n",
    "df_stats = pd.DataFrame(stats)\n",
    "display(df_stats)\n",
    "\n",
    "# Bar plot\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "x = np.arange(len(df_stats))\n",
    "ax.bar(x - 0.2, df_stats['train'], 0.4, label='Train (Normal)')\n",
    "ax.bar(x + 0.2, df_stats['defect'], 0.4, label='Test (Defect)', color='red')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_stats['category'], rotation=45, ha='right')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('MVTec AD Dataset Statistics')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'mvtec_statistics.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defect Type Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show different defect types for a category\n",
    "category = 'bottle'\n",
    "test_ds = MVTecDataset(category=category, split='test', return_mask=True)\n",
    "\n",
    "# Group by defect type\n",
    "defect_samples = {}\n",
    "for i in range(len(test_ds)):\n",
    "    defect_type = test_ds.get_defect_type(i)\n",
    "    if defect_type not in defect_samples:\n",
    "        defect_samples[defect_type] = i\n",
    "\n",
    "n_types = len(defect_samples)\n",
    "fig, axes = plt.subplots(3, n_types, figsize=(4*n_types, 10))\n",
    "\n",
    "for col, (dtype, idx) in enumerate(defect_samples.items()):\n",
    "    img, mask, label = test_ds[idx]\n",
    "    img_np = denormalize(img).permute(1, 2, 0).numpy().clip(0, 1)\n",
    "    mask_np = mask[0].numpy()\n",
    "    \n",
    "    axes[0, col].imshow(img_np)\n",
    "    axes[0, col].set_title(dtype.replace('_', ' ').title(), fontsize=11)\n",
    "    axes[0, col].axis('off')\n",
    "    \n",
    "    axes[1, col].imshow(mask_np, cmap='Reds')\n",
    "    axes[1, col].set_title('Defect Mask')\n",
    "    axes[1, col].axis('off')\n",
    "    \n",
    "    # Overlay\n",
    "    axes[2, col].imshow(img_np)\n",
    "    axes[2, col].imshow(mask_np, cmap='Reds', alpha=0.5)\n",
    "    axes[2, col].set_title('Overlay')\n",
    "    axes[2, col].axis('off')\n",
    "\n",
    "plt.suptitle(f'{category.title()} - Defect Types', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / f'{category}_defect_types.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Comparison (Load Trained Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained models (if available)\n",
    "category = 'bottle'\n",
    "models = {}\n",
    "\n",
    "# Try loading CAE\n",
    "cae_path = MODELS_DIR / f'cae_{category}_final.pth'\n",
    "if cae_path.exists():\n",
    "    cae = create_cae()\n",
    "    cae.load_state_dict(torch.load(cae_path, map_location=DEVICE)['model_state_dict'])\n",
    "    cae.eval()\n",
    "    models['CAE'] = cae\n",
    "    print('Loaded CAE')\n",
    "\n",
    "# Try loading VAE\n",
    "vae_path = MODELS_DIR / f'vae_{category}_final.pth'\n",
    "if vae_path.exists():\n",
    "    vae = create_vae()\n",
    "    vae.load_state_dict(torch.load(vae_path, map_location=DEVICE)['model_state_dict'])\n",
    "    vae.eval()\n",
    "    models['VAE'] = vae\n",
    "    print('Loaded VAE')\n",
    "\n",
    "# Try loading Denoising AE\n",
    "dae_path = MODELS_DIR / f'denoising_ae_{category}_final.pth'\n",
    "if dae_path.exists():\n",
    "    dae = create_denoising_ae()\n",
    "    dae.load_state_dict(torch.load(dae_path, map_location=DEVICE)['model_state_dict'])\n",
    "    dae.eval()\n",
    "    models['Denoising AE'] = dae\n",
    "    print('Loaded Denoising AE')\n",
    "\n",
    "if not models:\n",
    "    print('No trained models found. Please train models first using the training notebooks.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If models exist, compute scores and create comparison plots\n",
    "if models:\n",
    "    test_ds = MVTecDataset(category=category, split='test', return_mask=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_ds, batch_size=16, shuffle=False)\n",
    "    \n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        scores, labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for img, mask, label in test_loader:\n",
    "                img = img.to(DEVICE)\n",
    "                if name == 'VAE':\n",
    "                    score = model.get_anomaly_score(img)\n",
    "                else:\n",
    "                    score = model.get_reconstruction_error(img, reduction='mean')\n",
    "                scores.extend(score.cpu().numpy())\n",
    "                labels.extend(label.numpy())\n",
    "        results[name] = (np.array(labels), np.array(scores))\n",
    "    \n",
    "    # ROC curves comparison\n",
    "    plot_roc_curves(results, title=f'Model Comparison - {category.title()}',\n",
    "                    save_path=FIGURES_DIR / f'{category}_roc_comparison.png')\n",
    "    \n",
    "    # Precision-Recall curves\n",
    "    plot_precision_recall_curves(results, title=f'Precision-Recall - {category.title()}',\n",
    "                                  save_path=FIGURES_DIR / f'{category}_pr_comparison.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Anomaly Heatmap Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed heatmap analysis for one model\n",
    "if models:\n",
    "    model_name = list(models.keys())[0]\n",
    "    model = models[model_name]\n",
    "    \n",
    "    # Get samples\n",
    "    test_ds = MVTecDataset(category=category, split='test', return_mask=True)\n",
    "    \n",
    "    # Find defect samples\n",
    "    defect_indices = [i for i, l in enumerate(test_ds.labels) if l == 1][:5]\n",
    "    \n",
    "    for idx in defect_indices:\n",
    "        img, mask, label = test_ds[idx]\n",
    "        img_input = img.unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if model_name == 'VAE':\n",
    "                error_map = model.get_anomaly_map(img_input)[0]\n",
    "            else:\n",
    "                error_map = model.get_anomaly_map(img_input)[0]\n",
    "        \n",
    "        plot_anomaly_heatmap_overlay(\n",
    "            img, error_map, mask,\n",
    "            title=f'{model_name} - {test_ds.get_defect_type(idx)}',\n",
    "            save_path=FIGURES_DIR / f'{category}_heatmap_{idx}.png'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Score Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    for name, (labels, scores) in results.items():\n",
    "        normal_scores = scores[labels == 0]\n",
    "        anomaly_scores = scores[labels == 1]\n",
    "        \n",
    "        # Find optimal threshold\n",
    "        from sklearn.metrics import f1_score\n",
    "        thresholds = np.linspace(scores.min(), scores.max(), 100)\n",
    "        f1s = [f1_score(labels, scores > t) for t in thresholds]\n",
    "        best_thresh = thresholds[np.argmax(f1s)]\n",
    "        \n",
    "        plot_score_distribution(\n",
    "            normal_scores, anomaly_scores, threshold=best_thresh,\n",
    "            title=f'{name} Score Distribution - {category.title()}',\n",
    "            save_path=FIGURES_DIR / f'{category}_{name.lower().replace(\" \", \"_\")}_scores.png'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Latent Space Visualization (VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'VAE' in models:\n",
    "    vae = models['VAE']\n",
    "    test_ds = MVTecDataset(category=category, split='test', return_mask=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_ds, batch_size=16, shuffle=False)\n",
    "    \n",
    "    latent_vecs, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for img, mask, label in test_loader:\n",
    "            z = vae.encode(img.to(DEVICE))\n",
    "            latent_vecs.append(z.cpu())\n",
    "            labels.extend(label.numpy())\n",
    "    \n",
    "    latent_vecs = torch.cat(latent_vecs, 0).numpy()\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # PCA visualization\n",
    "    plot_latent_space_2d(latent_vecs, labels, method='pca',\n",
    "                         title=f'VAE Latent Space (PCA) - {category.title()}',\n",
    "                         save_path=FIGURES_DIR / f'{category}_vae_latent_pca.png')\n",
    "    \n",
    "    # t-SNE visualization (takes longer)\n",
    "    plot_latent_space_2d(latent_vecs, labels, method='tsne',\n",
    "                         title=f'VAE Latent Space (t-SNE) - {category.title()}',\n",
    "                         save_path=FIGURES_DIR / f'{category}_vae_latent_tsne.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multi-Category Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate CAE on multiple categories (if models trained)\n",
    "category_results = {}\n",
    "\n",
    "for cat in MVTEC_CATEGORIES[:5]:  # First 5 categories\n",
    "    model_path = MODELS_DIR / f'cae_{cat}_final.pth'\n",
    "    if model_path.exists():\n",
    "        model = create_cae()\n",
    "        model.load_state_dict(torch.load(model_path, map_location=DEVICE)['model_state_dict'])\n",
    "        model.eval()\n",
    "        \n",
    "        test_ds = MVTecDataset(category=cat, split='test', return_mask=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_ds, batch_size=16)\n",
    "        \n",
    "        scores, labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for img, mask, label in test_loader:\n",
    "                error = model.get_reconstruction_error(img.to(DEVICE), reduction='mean')\n",
    "                scores.extend(error.cpu().numpy())\n",
    "                labels.extend(label.numpy())\n",
    "        \n",
    "        auc = roc_auc_score(labels, scores)\n",
    "        category_results[cat] = auc\n",
    "        print(f'{cat}: AUC = {auc:.4f}')\n",
    "\n",
    "if category_results:\n",
    "    # Plot category results\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    cats = list(category_results.keys())\n",
    "    aucs = list(category_results.values())\n",
    "    colors = ['green' if a > 0.8 else 'orange' if a > 0.6 else 'red' for a in aucs]\n",
    "    \n",
    "    ax.bar(cats, aucs, color=colors)\n",
    "    ax.axhline(0.8, color='green', linestyle='--', alpha=0.5, label='Good (>0.8)')\n",
    "    ax.axhline(0.6, color='orange', linestyle='--', alpha=0.5, label='Fair (>0.6)')\n",
    "    ax.set_ylabel('ROC-AUC')\n",
    "    ax.set_title('CAE Performance by Category')\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'cae_category_comparison.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "print('='*60)\n",
    "print('ANALYSIS SUMMARY')\n",
    "print('='*60)\n",
    "\n",
    "# List all saved figures\n",
    "print('\\nGenerated Figures:')\n",
    "for f in sorted(FIGURES_DIR.glob('*.png')):\n",
    "    print(f'  - {f.name}')\n",
    "\n",
    "# List all models\n",
    "print('\\nTrained Models:')\n",
    "for f in sorted(MODELS_DIR.glob('*.pth')):\n",
    "    size_mb = f.stat().st_size / 1024 / 1024\n",
    "    print(f'  - {f.name} ({size_mb:.1f} MB)')"
   ]
  }
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}},
 "nbformat": 4,
 "nbformat_minor": 4
}
